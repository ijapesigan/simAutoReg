% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{FitVARLasso}
\alias{FitVARLasso}
\title{Fit Vector Autoregressive (VAR) Model Parameters using Lasso Regularization}
\usage{
FitVARLasso(Y_std, X_std, lambda, max_iter = 10000L, tol = 1e-05)
}
\arguments{
\item{Y_std}{Numeric matrix.
Matrix of standardized dependent variables (Y).}

\item{X_std}{Numeric matrix.
Matrix of standardized predictors (X).}

\item{lambda}{Lasso hyperparameter.
The regularization strength controlling the sparsity.}

\item{max_iter}{Integer.
The maximum number of iterations for the coordinate descent algorithm.
Default is 10000.}

\item{tol}{Numeric.
Convergence tolerance. The algorithm stops when the change in coefficients
between iterations is below this tolerance. Default is 1e-6.}
}
\value{
Matrix of estimated autoregressive and
cross-regression coefficients.
}
\description{
This function estimates the parameters of a VAR model
using the Lasso regularization method with cyclical coordinate descent.
The Lasso method is used to estimate the autoregressive
and cross-regression coefficients with sparsity.
}
\details{
The \code{\link[=FitVARLasso]{FitVARLasso()}} function estimates the parameters
of a Vector Autoregressive (VAR) model
using the Lasso regularization method.
Given the input matrices \code{Y_std} and \code{X_std},
where \code{Y_std} is the matrix of standardized dependent variables,
and \code{X_std} is the matrix of standardized predictors,
the function computes the autoregressive and cross-regression coefficients
of the VAR model with sparsity induced by the Lasso regularization.

The steps involved in estimating the VAR model parameters
using Lasso are as follows:
\itemize{
\item \strong{Initialization}: The function initializes the coefficient matrix
\code{beta} with OLS estimates.
The \code{beta} matrix will store the estimated autoregressive and
cross-regression coefficients.
\item \strong{Coordinate Descent Loop}: The function performs
the cyclical coordinate descent algorithm
to estimate the coefficients iteratively.
The loop iterates \code{max_iter} times (default is 10000),
or until convergence is achieved.
The outer loop iterates over the predictor variables
(columns of \code{X_std}),
while the inner loop iterates over the outcome variables
(columns of \code{Y_std}).
\item \strong{Coefficient Update}: For each predictor variable (column of \code{X_std}),
the function iteratively updates the corresponding column of \code{beta}
using the coordinate descent algorithm with L1 norm regularization
(Lasso).
The update involves calculating the soft-thresholded value \code{c},
which encourages sparsity in the coefficients.
The algorithm continues until the change in coefficients
between iterations is below the specified tolerance \code{tol}
or when the maximum number of iterations is reached.
\item \strong{Convergence Check}: The function checks for convergence
by comparing the current \code{beta}
matrix with the previous iteration's \code{beta_old}.
If the maximum absolute difference between \code{beta} and \code{beta_old}
is below the tolerance \code{tol},
the algorithm is considered converged, and the loop exits.
}
}
\examples{
Y_std <- StdMat(VAR_YX$Y)
X_std <- StdMat(VAR_YX$X[, -1])
lambda <- 73.90722
FitVARLasso(Y_std = Y_std, X_std = X_std, lambda = lambda)

}
\seealso{
The \code{\link[=FitVAROLS]{FitVAROLS()}} function for estimating VAR model parameters
using OLS.

Other Simulation of Autoregressive Data Functions: 
\code{\link{FitVARLassoSearch}()},
\code{\link{FitVAROLS}()},
\code{\link{LambdaSeq}()},
\code{\link{OrigScale}()},
\code{\link{PBootCI}()},
\code{\link{PBootSE}()},
\code{\link{PBootVAROLS}()},
\code{\link{SearchVARLasso}()},
\code{\link{SelectVARLasso}()},
\code{\link{SimAR}()},
\code{\link{SimMVN}()},
\code{\link{SimVARZIP}()},
\code{\link{SimVAR}()},
\code{\link{SimVariance}()},
\code{\link{StdMat}()},
\code{\link{YX}()}
}
\author{
Ivan Jacob Agaloos Pesigan
}
\concept{Simulation of Autoregressive Data Functions}
\keyword{fit}
\keyword{simAutoReg}
